{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras : 텐서플로우 코드를 좀 더 쉽게 작성할 수 있도록 해주는 라이브러리 , 추상화된 환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리 패키지 : nltk , konlpy 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "konlpy 는 자바 기반이라 [https://www.oracle.com/java/technologies/javase-downloads.html](https://www.oracle.com/java/technologies/javase-downloads.html)  \n",
    "\n",
    "- JDK windows x64 installer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 설치가 완료되면 JAVA_HOME 이라는 환경 변수를 글로벌에 추가하여 jdk-15.0.1 폴더 경로를 등록한다.  \n",
    "  \n",
    "  \n",
    "- 등록이 완료되면 자바와 파이썬을 연결해주는 jpype 설치 [https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype](https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype)    \n",
    "JPype1‑1.1.2‑cp38‑cp38‑win_amd64.whl : cp38 == python 3.8  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JPype1==1.1.2 from file:///C:/%EC%9E%84%EC%8B%9C/JPype1-1.1.2-cp37-cp37m-win_amd64.whl in c:\\users\\i\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\i\\anaconda3\\lib\\site-packages (from JPype1==1.1.2) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install C:/임시/JPype1-1.1.2-cp37-cp37m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"Don't After preventing President Obama from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"C:/임시/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 ? 자연어 처리 대상 (단어 , 문장 , 문단,..)  \n",
    "# 토큰 = 문법적으로 더 나눌 수 없는 요소  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'After', 'preventing', 'President', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'After', 'preventing', 'President', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr', '.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer # 특수기호에 대해 더 strict 하게 나눔\n",
    "print(WordPunctTokenizer().tokenize(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'after', 'preventing', 'president', 'obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', 'republicans', 'moved', 'swiftly', 'to', 'deliver', 'president', 'trump', 'a', 'victory', 'days', 'before', 'the', 'election', 'both', 'presidential', 'candidates', 'campaigned', 'in', 'pennsylvania', 'where', 'joe', 'biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'mr', 'trump', 'mocked', 'kamala', 'harris']\n"
     ]
    }
   ],
   "source": [
    "print(tts(news)) # 자체적으로 소문자 통일 , 혼자 쓰이는 특수문자들을 제거해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't After preventing President Obama from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election.\", 'Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) __형태소 : 자립형태소 , 의존형태소(조사)__  \n",
    "ex) 길동이가 딥러닝을 합니다.  \n",
    "  \n",
    "2) __띄어쓰기 : 지금이문장을해석하는데어려움이있나요?__  \n",
    "ex) 아버지가방에들어가신다.\n",
    "    \n",
    "3) __품사에 따라 단어의 의미가 달라짐__  \n",
    "ex) mine : 지뢰 , 광물을 캐다. => 품사 태깅을 미리 수행하여, 특정 단어가 사용되기 앞서 어떤 품사로 사용되는지 구분. tagging nl  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'After preventing President Obama from filling a vacancy four years ago.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'preventing', 'President', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('After', 'IN'),\n",
       " ('preventing', 'VBG'),\n",
       " ('President', 'NNP'),\n",
       " ('Obama', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('filling', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('vacancy', 'NN'),\n",
       " ('four', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "pos_tag(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag 확인\n",
    "# konlpy.org\n",
    "# https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '공부', '하고', '있는', '우리', '들', ',', '수료', '후', '에는', '취업', '에', '꼭', '성공해요']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석? => 형태소 단위로 토큰화 한다는 의미\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs('열심히 공부하고 있는 우리들, 수료 후에는 취업에 꼭 성공해요'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 자연어 전처리 , 언어 모델  \n",
    "2. 문서단어벡터 (tfidf) , 문서 유사도  \n",
    "3. 토픽 모델링 (LDA)  \n",
    "4. 머신러닝 기반 텍스트 분류 ------------------  \n",
    "5. rnn -> lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 마이닝 사전 작업\n",
    "\n",
    "# 1.텍스트 마이닝에서 데이터 정제 : 코퍼스로부터 노이즈 데이터를 제거하는 작업\n",
    "# ex. 인공지능 코퍼스에 연필이라는 단어가 들어왔을때 관련 없으므로 제거\n",
    "\n",
    "# 2.텍스트 마이닝에서 데이터 정규화 : 동의어들을 같은 단어로 통합하는 작업 \n",
    "# ex. 뛴다 ,뛰었다, 뛴고 있다 => 뛰다.\n",
    "\n",
    "# 3.텍스트 마이닝에서 불용어 제거 \n",
    "# 4.등장 빈도수가 낮은 단어 제거 (연산량 , 속도 개선)\n",
    "# 5.길이가 짧은 단어 제거\n",
    "# 6.정규표현식 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__어간 추출 (stemming) 2가지 방법__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__method #1__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표제어 추출 함수 : lemmatize \n",
    "# 표제어? == 단어 기본형\n",
    "wnl.lemmatize('were')\n",
    "wnl.lemmatize('has','v')\n",
    "wnl.lemmatize('dies','v')\n",
    "wnl.lemmatize('watched','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['live', 'go', 'expect', 'do']\n"
     ]
    }
   ],
   "source": [
    "res = [wnl.lemmatize(w,'v') for w in ['lives','going','expected','done']]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__method #2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"After preventing President Obama from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'preventing', 'President', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'prevent', 'presid', 'obama', 'from', 'fill', 'a', 'vacanc', 'four', 'year', 'ago', ',', 'republican', 'move', 'swiftli', 'to', 'deliv', 'presid', 'trump', 'a', 'victori', 'day', 'befor', 'the', 'elect', '.', 'both', 'presidenti', 'candid', 'campaign', 'in', 'pennsylvania', ',', 'where', 'joe', 'biden', 'said', 'he', 'would', 'expand', 'hi', 'elector', 'map', 'and', 'mr.', 'trump', 'mock', 'kamala', 'harri', '.']\n"
     ]
    }
   ],
   "source": [
    "res = [ps.stem(w) for w in word_tokenize(text)]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aft', 'prev', 'presid', 'obam', 'from', 'fil', 'a', 'vac', 'four', 'year', 'ago', ',', 'republ', 'mov', 'swift', 'to', 'del', 'presid', 'trump', 'a', 'vict', 'day', 'bef', 'the', 'elect', '.', 'both', 'presid', 'candid', 'campaign', 'in', 'pennsylvan', ',', 'wher', 'joe', 'bid', 'said', 'he', 'would', 'expand', 'his', 'elect', 'map', 'and', 'mr.', 'trump', 'mock', 'kamal', 'har', '.']\n"
     ]
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "res = [ls.stem(w) for w in word_tokenize(text)]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 불용어 \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = \"Family is not an important thing. It's everything.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = word_tokenize(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "important\n",
      "thing\n",
      ".\n",
      "It\n",
      "'s\n",
      "everything\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in wt:\n",
    "    if w not in sw:\n",
    "        print(w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '텐서플로우와 머신러닝으로 자연어 처리를 하거든. 머신러닝으로만 자연어 처리를 하면 안돼.'\n",
    "st_words = '어째든 아무거나 같다 비슷하다 하면 하거든 으로'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = st_words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['텐서플로우와',\n",
       " '머신러닝으로',\n",
       " '자연어',\n",
       " '처리를',\n",
       " '하거든',\n",
       " '.',\n",
       " '머신러닝으로만',\n",
       " '자연어',\n",
       " '처리를',\n",
       " '하면',\n",
       " '안돼',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt = word_tokenize(data)\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['텐서플로우와', '머신러닝으로', '자연어', '처리를', '.', '머신러닝으로만', '자연어', '처리를', '안돼', '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in sw_list:\n",
    "        res.append(w)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어 사전 사이트 [https://www.ranks.nl/stopwords/korean](https://www.ranks.nl/stopwords/korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "전화번호 : 010-3109-2289\n",
    "주소 : 서울시 강남구\n",
    "나이 : 28\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['전화번호', '주소', '서울시', '강남구', '나이']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 숫자들만 모두 추출\n",
    "\n",
    "pat = '[가-히]+'\n",
    "re.findall(pat,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '대한민국 한국 코리아 korea 남한 고려'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국 대한민국 코리아 대한민국 대한민국 대한민국'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동의어 대체\n",
    "# re.sub('정규표현식',치환문자,변수)\n",
    "re.sub('한국','대한민국',text)\n",
    "re.sub('한국|korea|남한|고려','대한민국',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Her impact could be felt right away. There are major election disputes awaiting immediate action by the Supreme Court from the battleground states of North Carolina and Pennsylvania. Both concern the date by which absentee ballots may be accepted. With Justice Barrett’s elevation in place of Justice Ginsburg, a liberal icon, the court is expected to tilt decisively to the right. It is gaining a conservative who could sway cases in every area of American life, including abortion rights, gay rights, business regulation and the environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her impact could be felt right away.',\n",
       " 'There are major election disputes awaiting immediate action by the Supreme Court from the battleground states of North Carolina and Pennsylvania.',\n",
       " 'Both concern the date by which absentee ballots may be accepted.',\n",
       " 'With Justice Barrett’s elevation in place of Justice Ginsburg, a liberal icon, the court is expected to tilt decisively to the right.',\n",
       " 'It is gaining a conservative who could sway cases in every area of American life, including abortion rights, gay rights, business regulation and the environment.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = sent_tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 크리닝(정제 작업) , 단어 토큰화\n",
    "# 문장 -> 단어 -> 소문자 통일 -> 불용어 제거 -> 짧은 길이의 단어 제거.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['impact', 'could', 'felt', 'right', 'away'], ['major', 'election', 'disputes', 'awaiting', 'immediate', 'action', 'supreme', 'court', 'battleground', 'states', 'north', 'carolina', 'pennsylvania'], ['concern', 'date', 'absentee', 'ballots', 'may', 'accepted'], ['justice', 'barrett', 'elevation', 'place', 'justice', 'ginsburg', 'liberal', 'icon', 'court', 'expected', 'tilt', 'decisively', 'right'], ['gaining', 'conservative', 'could', 'sway', 'cases', 'every', 'area', 'american', 'life', 'including', 'abortion', 'rights', 'gay', 'rights', 'business', 'regulation', 'environment']]\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "vocab={}\n",
    "sentences = []\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i)\n",
    "    res=[]\n",
    "    for word in sentence :\n",
    "        word = word.lower() # 소문자화\n",
    "        if (word not in stopWords) and (len(word)>2):\n",
    "            res.append(word)\n",
    "            if word not in vocab:\n",
    "                vocab[word]=0 \n",
    "            vocab[word]+=1\n",
    "    sentences.append(res)\n",
    "print(sentences)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'impact': 1, 'could': 2, 'felt': 1, 'right': 2, 'away': 1, 'major': 1, 'election': 1, 'disputes': 1, 'awaiting': 1, 'immediate': 1, 'action': 1, 'supreme': 1, 'court': 2, 'battleground': 1, 'states': 1, 'north': 1, 'carolina': 1, 'pennsylvania': 1, 'concern': 1, 'date': 1, 'absentee': 1, 'ballots': 1, 'may': 1, 'accepted': 1, 'justice': 2, 'barrett': 1, 'elevation': 1, 'place': 1, 'ginsburg': 1, 'liberal': 1, 'icon': 1, 'expected': 1, 'tilt': 1, 'decisively': 1, 'gaining': 1, 'conservative': 1, 'sway': 1, 'cases': 1, 'every': 1, 'area': 1, 'american': 1, 'life': 1, 'including': 1, 'abortion': 1, 'rights': 2, 'gay': 1, 'business': 1, 'regulation': 1, 'environment': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "sentences # 특정 분야의 코퍼스라 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "# 코퍼스에 등장하는 단얻르에 대한 빈도수를 기준으로 단어 집합 생성\n",
    "# 빈도수가 높은~낮은 단어 순으로 번호를 부여 -> 확인은 word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 5 # 가장 빈도수가 높은 5개 단어만 추출\n",
    "tokenizer = Tokenizer(num_words=vs+1)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5],\n",
       " [1, 3, 5],\n",
       " [2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4],\n",
       " [1, 4],\n",
       " [1, 4, 2],\n",
       " [3, 2, 1],\n",
       " [1, 3]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences) # 5번 이후로 밀린 단어들은 포함하지 않음?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
