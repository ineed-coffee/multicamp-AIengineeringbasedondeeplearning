library(readxl)
sms_raw <- read.csv('C:/임시/RData/sms_spam_ansi.txt')
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw)
table(sms_raw$type)
install.packages('tm')
library(tm)
tm::stopwords('en')
tm::removeWords('of so many people',stopwords('en'))
IMDB <- read.csv('C:/임시/RData/IMDB-Movie-Data.csv')
str(IMDB)
summary(IMDB)
sum(is.na(IMDB$Metascore))
colSums(is.na(IMDB))
IMDB2 <- na.omit(IMDB)
colSums(is.na(IMDB2))
# 특정 변수에 결측값이 있는 경우만 행을 삭제하고자 한다면
IMDB[complete.cases(IMDB[,12]),]
# 특정 변수에 결측값이 있는 경우만 행을 삭제하고자 한다면
IMDB3 <- IMDB[complete.cases(IMDB[,12]),]
colSums(is.na(IMDB3))
# 1분위 수
Q1 <- quantile(IMDB$Revenue..Millions.,probs=c(0.25),na.rm = T)
Q3 <- quantile(IMDB$Revenue..Millions.,probs=c(0.75),na.rm = T)
IQR <- Q3-Q1
LC <- Q1-(1.5*IQR)
UC <- Q3+(1.5*IQR)
# 정상범위 추출 : subset
IMDB2 <-subset(IMDB, IMDB$Revenue..Millions.>LC&IMDB$Revenue..Millions.<UC)
# 부분 문자열 추출 : substr
IMDB$Actors[1]
substr(IMDB$Actors[1],1,5)
# 문자열 붙이기
paste(IMDB$Actors[1],'_',"A")
# 문자열 분할
strsplit(IMDB$Actors[1],split=',')
# 문자열 대체
gsub(","," ",IMDB$Genre)
IMDB$Genre2 <-gsub(","," ",IMDB$Genre)
Corpus(VectorSource(IMDB$Genre2))
CORPUS <- Corpus(VectorSource(IMDB$Genre2))
# 특수문자 제거 , 숫자 , 소문자 통일
CORPUS_tm <- tm_map(CORPUS,removePunctuation) # 특수문자 제거
tm_map(CORPUS_tm,removeNumbers) # 숫자 제거
tm_map(CORPUS_tm,tolower) # 소문자 통일
# DTM 생성
DTM <- DocumentTermMatrix(CORPUS_tm)
DTM
inspect(DTM)
as.data.frame(as.matrix(DTM))
DTM <- as.data.frame(as.matrix(DTM))
cbind(IMDB,DTM)
IMDB_Genre <-cbind(IMDB,DTM)
IMDB$Description
CORPUS <- Corpus(VectorSource(IMDB$Description))
CORPUS_tm <- tm_map(CORPUS,stripWhitespace) # 공백문자 제거
CORPUS_tm <- tm_map(CORPUS_tm,removeNumbers) # 숫자자문자 제거
CORPUS_tm <- tm_map(CORPUS_tm,tolower) # 소문자 통일
CORPUS_tm <- tm_map(CCORPUS_tm,removePunctuation) # 구두점 제거
DTM <- DocumentTermMatrix(CORPUS_tm)
CORPUS_tm <- tm_map(CORPUS_tm,removePunctuation) # 구두점 제거
DTM <- DocumentTermMatrix(CORPUS_tm)
inspect(DTM)
IMDB$Description[155]
CORPUS_tm <- tm_map(CORPUS_tm,removeWords,c(stopwords('en'),'my','custom','words'))
TDM <- TermDocumentMatrix(DTM)
TDM <- TermDocumentMatrix(CORPUS_tm)
m <- as.matrix(TDM)
rowSums(m)
sort(rowSums(m))
sort(rowSums(m))
sort(rowSums(m),decreasing = T)
v <- sort(rowSums(m),decreasing = T)
names(v)
data.frame(word=names(v),freq=v)
d <- data.frame(word=names(v),freq=v)
install.packages('Snowballc') # 어근 추출 패키지
install.packages('SnowballC') # 어근 추출 패키지
library(SnowballC)
install.packages('wordcloud')
library(wordcloud)
library(wordcloud)
install.packages(RColorsBrewer)
install.packages(RColorBrewer)
install.packages('RColorBrewer')
install.packages("RColorBrewer")
library(RColorBrewer)
wordcloud(words=d$word, freq=d$freq , min.freq=5,
max.words=200,random.order=F,
colors=brewer.pal(8,'Dark2'))
library(wordcloud)
wordcloud(words=d$word, freq=d$freq , min.freq=5,
max.words=200,random.order=F,
colors=brewer.pal(8,'Dark2'))
sms_raw$text
library(tm)
library(tm)
Corpus(VectorSource(sms_raw$text))
VCorpus(VectorSource(sms_raw$text))
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
sms_corpus
inspect(sms_corpus)
sms_corpus[[1]]
class(sms_corpus[[1]])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:3],as.character)
sms_corpus_clean <- tm_map(sms_corpus,removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus,removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean,content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean,removePunctuation)
sms_corpus_clean <- tm_map(sms_corpus_clean,removeWords,stopwords())
removePunctuation('hello..., ; : hihihi')
myreplace <- function(x){
gsub("[[:punct:]]+"," ",x)
}
myreplace('hello...,..world')
library(SnowballC)
wordStem(c('learned','learn','learning','learns'))
tm_map(c('learned','learn','learning','learns'),stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean,stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean,stripWhitespace)
lapply(sms_[1:3],as.character)
lapply(sms_corpus_clean[1:3],as.character)
lapply(sms_corpus[1:3],as.character)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm)
sms_train_labels <- sms_raw[1:4169,]$type # 학습 정담담
sms_train_labels <- sms_raw[1:4169,]$type # 학습 정답
sms_test_labels <- sms_raw[4170:5559,]$type # 예측 정답
sms_dtm_train <- sms_dtm[1:4169]
sms_dtm_test <- sms_dtm[4170:5559]
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5559,]
table(sms_train_labels)
table(sms_test_labels)
table(sms_train_labels)
prop.table(sms_test_labels)
prop.table(table(sms_test_labels))
wordcloud(sms_corpus_clean,min.freq = 50,random.order = F)
subset(sms_raw,type=='spam')
spam <- subset(sms_raw,type=='spam')
ham <- subset(sms_raw,type=='ham')
wordcloud(spam$text , max.words = 40)
wordcloud(ham$text, max.words = 50)
wordcloud(spam$text , max.words = 40)
wordcloud(ham$text, max.words = 50)
wordcloud(spam$text , max.words = 40)
wordcloud(ham$text, max.words = 50)
install.packages('e1071')
library(e1071)
sms_dtm_freq_train <- removeSparseTerms(sms_dtm_train,0.999)
sms_dtm_freq_train
sms_dtm_train
findFreqTerms(sms_dtm_train,5)
sms_freq_words <- findFreqTerms(sms_dtm_train,5)
str(sms_freq_words)
sms_dtm_train[,sms_freq_words]
sms_dtm_freq_train<- sms_dtm_train[,sms_freq_words]
sms_dtm_freq_test<- sms_dtm_test[,sms_freq_words]
inspect(sms_dtm_freq_train)
convertCounts <- function(x){
x<-ifelse(x>0,'YES','NO')
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convertCounts)
sms_train
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convertCounts)
sms_train[,1]
str(sms_train)
sms_Classifier <- naiveBayes(sms_train,sms_train_labels)
sms_test_pred <- predict(sms_Classifier,sms_test)
library(gmodels)
CrossTable(sms_test_pred,sms_test_labels)
CrossTable(sms_test_pred,sms_test_labels,dnn=c('predicted','actual'))
sms_Classifier <- naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_test_pred <- predict(sms_Classifier,sms_test)
CrossTable(sms_test_pred,sms_test_labels,
dnn=c('predicted','actual'))
print((1189+166)/1390)
library(readxl)
sms_train <- read.csv('C:/임시/RData/sms_spam_ansi.txt')
sms_test <- read.csv('C:/임시/RData/햄스팸테스트.txt')
# 병합 후 전처리 -> 끝나면 다시 분할
dim(sms_train)
# 병합 후 전처리 -> 끝나면 다시 분할
dim(sms_train)[1]
# 병합 후 전처리 -> 끝나면 다시 분할
split_point <- dim(sms_train)[1]
class(split_point)
data <- cbind(sms_train,sms_test)
data <- rbind(sms_train,sms_test)
names(sms_train)
names(sms_test)
sms_test <- read.csv('C:/임시/RData/햄스팸테스트.txt',header=F)
names(sms_test)
names(sms_test) <- c('type','text')
names(sms_train)
data <- rbind(sms_train,sms_test) # 병합
table(data$type) # 4812 hams 747 spams
library(tm)
library(SnowballC)
wordStem(c('learned','learn','learning','learns')) #1
tm_map(c('learned','learn','learning','learns'),stemDocument)
sms_corpus <- VCorpus(VectorSource(data$text)) # 코퍼스 생성
sms_corpus_preprocessed <- tm_map(sms_corpus,removeNumbers) #
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,content_transformer(tolower))
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removePunctuation)
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removeWords,stopwords())
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,wordStem)
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_preprocessed)
sms_corpus <- VCorpus(VectorSource(data$text)) # 코퍼스 생성
sms_corpus_preprocessed <- tm_map(sms_corpus,removeNumbers) # 숫자 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,content_transformer(tolower)) # 소문자 통일
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removePunctuation) # 특수문자 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removeWords,stopwords()) # 불용어 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,stemDocument) # 어근 변환
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,stripWhitespace) # 공백 제거
sms_dtm <- DocumentTermMatrix(sms_corpus_preprocessed)
library(e1071)
inspect(sms_dtm)
sms_dtm
sms_dtm <- sms_dtm[freq]
# 희소성 리미팅
freq <-removeSparseTerms(sms_dtm_train,0.999)
sms_dtm <- sms_dtm[freq]
# 희소성 리미팅
freq <-removeSparseTerms(sms_dtm,0.999)
sms_dtm <- sms_dtm[freq]
sms_dtm <- sms_dtm[,freq]
sms_dtm
sms_dtm <- DocumentTermMatrix(sms_corpus_preprocessed)
sms_dtm # Non-/sparse entries: 43348/38416166    #Sparsity: 100%
# 희소성 리미팅
freq <-removeSparseTerms(sms_dtm,0.999)
sms_dtm <- sms_dtm[,freq]
sms_dtm
freq
sms_dtm <- DocumentTermMatrix(sms_corpus_preprocessed)
sms_dtm # Non-/sparse entries: 43348/38416166    #Sparsity: 100%
sms_dtm <-removeSparseTerms(sms_dtm,0.999)
sms_dtm
# 빈도 수치 범주형 변환
sms_dtm <- apply(sms_dtm, MARGIN = 2, function(x)ifelse(x>0,'Y','N'))
str(sms_dtm)
xtrain <- sms_dtm[1:split_point,]
ytrain <- data$type[1:split_point]
xtest <- sms_dtm[split_point:5569,]
ytest <- data$type[split_point:5569]
sms_Classifier <- naiveBayes(x_train,y_train,laplace = 1)
sms_Classifier <- naiveBayes(xtrain,ytrain,laplace = 1)
library(gmodels)
y_pred <- predict(sms_Classifier,xtest)
ypred <- predict(sms_Classifier,xtest)
# 성능 출력
CrossTable(ypred,ytest,
dnn=c('predicted','actual'))
ytrain <- factor(data$type[1:split_point])
ytest <- factor(data$type[split_point:5569])
# 베이시안 분류기 생성&예측
sms_Classifier <- naiveBayes(xtrain,ytrain,laplace = 1)
ypred <- predict(sms_Classifier,xtest)
# 성능 출력
CrossTable(ypred,ytest,
dnn=c('predicted','actual'))
# 성능 출력
CrossTable(ypred,ytest,
dnn=c('predicted','actual'))
# 스팸 메일 분류기를 통해 예측하고 결과 혼동행렬을 출력해보기
library(readxl)
library(tm)
library(SnowballC)
library(e1071)
library(gmodels)
sms_train <- read.csv('C:/임시/RData/sms_spam_ansi.txt')
sms_test <- read.csv('C:/임시/RData/햄스팸테스트.txt',header=F)
# 병합 후 전처리 -> 끝나면 다시 분할
split_point <- dim(sms_train)[1]
names(sms_test) <- c('type','text') # 칼럼명 통일 (병합에 필요)
data <- rbind(sms_train,sms_test) # 병합
table(data$type) # 4818 hams 751 spams
# 전처리 -------------------------------------------
sms_corpus <- VCorpus(VectorSource(data$text)) # 코퍼스 생성
sms_corpus_preprocessed <- tm_map(sms_corpus,removeNumbers) # 숫자 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,content_transformer(tolower)) # 소문자 통일
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removePunctuation) # 특수문자 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removeWords,stopwords()) # 불용어 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,stemDocument) # 어근 변환
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,stripWhitespace) # 공백 제거
# 문서 단어 행렬 생성 ----------------------------------
sms_dtm <- DocumentTermMatrix(sms_corpus_preprocessed)
sms_dtm #terms :6909 #Non-/sparse entries: 43348/38416166    #Sparsity: 100%
# 희소성 리미팅
sms_dtm <-removeSparseTerms(sms_dtm,0.999)
sms_dtm #terms :1200 #Non-/sparse entries: 34159/6648641     #Sparsity: 99%
# 빈도 수치 범주형 변환
sms_dtm <- apply(sms_dtm, MARGIN = 2, function(x)ifelse(x>0,'Y','N'))
# 학습,예측 데이터 분할
xtrain <- sms_dtm[1:split_point,]
ytrain <- factor(data$type[1:split_point])
xtest <- sms_dtm[split_point+1:5569,]
ytest <- factor(data$type[split_point+1:5569])
# 베이시안 분류기 생성&예측
sms_Classifier <- naiveBayes(xtrain,ytrain,laplace = 1)
ypred <- predict(sms_Classifier,xtest)
# 성능 출력
CrossTable(ypred,ytest,
dnn=c('predicted','actual'))
# 정확도 100%
# 정확도 100%
split_point+1
data$type[5560:5569]
sms_dtm[split_point+1:5569,]
sms_dtm[5560]
sms_dtm[(split_point+1):5569,]
sms_dtm[5560]
sms_dtm[5560,]
sms_dtm[5560:5569,]
# 스팸 메일 분류기를 통해 예측하고 결과 혼동행렬을 출력해보기
library(readxl)
library(tm)
library(SnowballC)
library(e1071)
library(gmodels)
sms_train <- read.csv('C:/임시/RData/sms_spam_ansi.txt')
sms_test <- read.csv('C:/임시/RData/햄스팸테스트.txt',header=F)
# 병합 후 전처리 -> 끝나면 다시 분할
split_point <- dim(sms_train)[1]
names(sms_test) <- c('type','text') # 칼럼명 통일 (병합에 필요)
data <- rbind(sms_train,sms_test) # 병합
table(data$type) # 4818 hams 751 spams
# 전처리 -------------------------------------------
sms_corpus <- VCorpus(VectorSource(data$text)) # 코퍼스 생성
sms_corpus_preprocessed <- tm_map(sms_corpus,removeNumbers) # 숫자 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,content_transformer(tolower)) # 소문자 통일
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removePunctuation) # 특수문자 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,removeWords,stopwords()) # 불용어 제거
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,stemDocument) # 어근 변환
sms_corpus_preprocessed <- tm_map(sms_corpus_preprocessed,stripWhitespace) # 공백 제거
# 문서 단어 행렬 생성 ----------------------------------
sms_dtm <- DocumentTermMatrix(sms_corpus_preprocessed)
sms_dtm #terms :6909 #Non-/sparse entries: 43348/38416166    #Sparsity: 100%
# 희소성 리미팅
sms_dtm <-removeSparseTerms(sms_dtm,0.999)
sms_dtm #terms :1200 #Non-/sparse entries: 34159/6648641     #Sparsity: 99%
# 빈도 수치 범주형 변환
sms_dtm <- apply(sms_dtm, MARGIN = 2, function(x)ifelse(x>0,'Y','N'))
# 학습,예측 데이터 분할
xtrain <- sms_dtm[1:split_point,]
ytrain <- factor(data$type[1:split_point])
xtest <- sms_dtm[(split_point+1):5569,]
ytest <- factor(data$type[(split_point+1):5569])
# 베이시안 분류기 생성&예측
sms_Classifier <- naiveBayes(xtrain,ytrain,laplace = 1)
ypred <- predict(sms_Classifier,xtest)
# 성능 출력
CrossTable(ypred,ytest,
dnn=c('predicted','actual'))
# 정확도 100%
library(readxl)
library(dplyr)
data <- read.csv('C:/임시/RData/credit.csv')
str(data)
credit <- read.csv('C:/임시/RData/credit.csv')
str(credit)
credit <- read.csv('C:/임시/RData/credit.csv')
str(credit)
table(credit$checking_balance)
table(credit$saving_balance) # 계좌 잔고
table(credit$savings_balance) # 저축 잔고
summary(credit$months_loan_duration)
summary(credit$amount) # 대출금 4개월 ~ 72개월
table(credit$default)
# 훈련:예측 = 9:1
set.seed(123)
sample(10,5)
sample(10,5)
trainSample <- sample(1000,900)
str(trainSample)
str(credit)
creditTrain <- credit[trainSample,]
creditTest <- credit[-trainSample,]
table(creditTrain$default)
table(creditTest$default)
install.packages('c50')
install.packages('C50')
library(C50)
str(credit)
creditTrain$default <- factor(creditTrain$default)
str(creditTrain)
model <- C5.0(creditTrain[-17],creditTrain$default,trials = 50)
model
creditPred <- predict(model,creditTest)
library(tm)
library(gmodels)
CrossTable(creditTest$default,creditPred,
dnn=c('actual','predicted'))
